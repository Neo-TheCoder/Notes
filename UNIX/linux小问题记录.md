
# malloc分配方式
在C/C++中，`malloc` 函数用于动态分配内存空间。具体分配内存的方式是在堆（Heap）中找到一块足够大小的连续空闲内存块，然后将该内存块标记为已使用，并返回该内存块的起始地址给调用者。`malloc` 函数的原型为：

```c
void* malloc(size_t size);
```

`malloc` 函数接受一个参数 `size`，表示需要分配的内存空间大小（以字节为单位）。它会在堆中找到一块大小不小于 `size` 的空闲内存块，如果找到合适的内存块，则返回该内存块的起始地址；如果找不到足够大的内存块，则返回 `NULL`。

`malloc` 函数的一些缺点包括：
1. **内存泄漏**：使用 `malloc` 分配内存后，需要手动调用 `free` 函数来释放内存。如果忘记释放内存，就会导致内存泄漏，使得程序占用的内存不断增加。
2. **无法自动调整内存大小**：`malloc` 分配的内存大小是固定的，无法动态调整。如果需要调整内存大小，需要重新分配内存并拷贝数据，比较繁琐。
3. **没有类型检查**：`malloc` 返回的是 `void*` 类型的指针，需要手动进行类型转换，容易出现类型错误。

## 传一个指针进来，它如何确定具体要清理多少空间？
当传入一个指针进来时，程序无法确定具体要清理多少空间。通常情况下，需要在调用 `malloc` 分配内存的同时记录下分配的大小，以便在释放内存时使用。一种常见的做法是在分配内存时多分配一些空间用于存储分配的大小，然后在释放内存时先读取这个大小，再释放相应大小的内存。这样可以确保正确地释放分配的内存空间，避免内存泄漏和越界访问等问题。




# linux如何从用户态转到内核态？
在Linux系统中，从用户态（User Mode）转换到内核态（Kernel Mode）通常涉及系统调用（System Call）或者异常（Exception）的处理。以下是一般情况下的用户态到内核态的转换方式：

1. **系统调用（System Call）**：
   - 用户程序通过`调用库函数`（如C标准库中的函数）发起系统调用，比如：打开文件、读写文件等操作。
   - `库函数`会将`系统调用号`和`参数`传递给内核，然后触发`软中断`（Software Interrupt）。
   - `操作系统内核`会根据`系统调用号`找到对应的`处理函数`，并在**内核态**执行该函数。
   - 在`内核态`执行完`系统调用`后，将结果返回给用户程序。

2. **异常（Exception）**：
   - 当用户程序执行一些特殊的指令或者发生一些特定的情况时（如除零错误、访问非法内存等），会触发异常。
   - 异常会导致`处理器`从`用户态`切换到`内核态`，并跳转到`内核`中预先定义好的`异常处理程序`。
   - 内核态的异常处理程序会根据异常类型进行相应的处理，可能会终止用户程序、发送信号等。

总的来说，无论是系统调用还是异常，都会导致**处理器**从**用户态切换到内核态**，这涉及到特权级别的切换和硬件的支持。
在内核态中，操作系统具有更高的权限和更广泛的访问权限，可以执行一些用户态无法执行的操作，如访问硬件设备、管理内存等。


# 一次I/O操作的完整流程
一次I/O（Input/Output）操作的完整流程涉及用户程序、操作系统内核和硬件设备之间的交互。以下是一次I/O 操作的典型流程：

1. **用户程序发起系统调用**：
   - 用户程序通过调用相应的系统调用接口（如`read()`或`write()`）请求进行I/O 操作。
   - 系统调用会将请求传递给操作系统内核。

2. **内核准备I/O 操作**：
   - 操作系统内核接收到用户程序的I/O 请求后，会进行相应的准备工作，包括确定要操作的设备、数据传输方向等。

3. **内核发起设备控制命令**：
   - 内核根据用户程序的请求向相应的设备控制器发送命令，要求设备执行读取或写入操作。

4. **设备执行I/O 操作**：
   - 设备控制器接收到内核发送的命令后，开始执行实际的 I/O 操作，包括从设备读取数据或向设备写入数据。

5. **数据传输**：
   - 如果是读取操作，设备会将数据读取到设备缓冲区中；如果是写入操作，设备会将数据从设备缓冲区写入到设备中。

6. **中断通知**：
   - 当设备完成数据传输后，会向设备控制器发送中断信号，通知操作系统内核数据已准备好。

7. **内核处理中断**：
   - 操作系统内核接收到设备的中断信号后，会进行中断处理程序的调用，处理设备完成的I/O 操作。

8. **数据传递给用户空间**：
   - 如果是读取操作，内核会将数据从设备缓冲区复制到用户程序的内存空间；如果是写入操作，内核会将用户程序提供的数据复制到设备缓冲区。

9. **系统调用返回**：
   - 内核将操作结果返回给用户程序，系统调用完成，用户程序继续执行后续操作。

整个I/O 操作的流程涉及用户程序、操作系统内核和硬件设备之间的协同工作，通过系统调用、中断处理等机制实现数据的传输和交互。



# DMA
DMA（Direct Memory Access，直接内存访问）是计算机系统中一种数据传输方式，允许外部设备直接访问计算机内存，而无需经过中央处理器（CPU）的介入。以下是关于DMA的详细说明：

1. **工作原理**：
   - 在传统的I/O操作中，数据传输通常需要通过CPU来控制，即CPU从外部设备读取数据或向外部设备写入数据。而使用DMA时，外部设备可以直接与内存进行数据传输，而不需要CPU的直接参与。
   - DMA控制器是负责管理数据传输的硬件设备，它可以独立地访问系统总线，直接和内存进行数据交换。

2. **优点**：
   - 提高系统性能：通过使用DMA，可以减少CPU的负担，释放CPU用于执行其他任务，从而提高系统的整体性能。
   - 高效的数据传输：DMA能够实现高速、连续的数据传输，提高数据传输的效率和速度。

3. **应用**：
   - DMA常用于大数据量的数据传输，比如网络数据包的接收和发送、磁盘数据的读写等。
   - 在图形处理、音频处理等需要高速数据传输的应用中，DMA也发挥着重要作用。

4. **工作流程**：
   - 外部设备向DMA控制器发送数据传输请求。
   - DMA控制器获取总线控制权，直接访问内存，将数据传输到指定内存地址或从指定内存地址读取数据。
   - 数据传输完成后，DMA控制器会向外部设备发送传输完成的信号。

总的来说，DMA是一种通过硬件实现的数据传输方式，能够提高系统性能，减少CPU的负担，适用于需要高效数据传输的场景，如大数据量的数据传输和高速数据处理。



# CGroup
linux中，有将进程分组的概念以及需求，经常需要追踪一组进程的内存和IO的使用情况。

CGroup的功能：**对一组进程进行分组，以及对进程进行统一的资源监控和限制**
用户层看来，cgroup就是把系统中的进程组织成若干树，**树的每个节点是一个进程组**，每棵树和一个或多个`subsystem`关联，树的作用是将进程进行分组，而`subsystem`的作用就是对这些组进行操作。


## 组成部分
### subsystem
一个subsystem就是一个**内核模块**，他被关联到一颗cgroup树之后，就会在树的每个节点(进程组) 上做具体的操作。
subsystem经常被称作"resource controller”，因为它主要被用来调度或者限制每个进程组的资源，但是这个说法不完全准确，因为有时我们将进程分组只是为了做一些监控，观察一下他们的状态，比如：perf event subsystem。


### hierarchy
一个hierarchy可以理解为一棵cgroup树，树的每个节点就是一个进程组，每棵树都会与零到多个subsystem关联。
系统中可以有很多颗cgroup树，每棵树都和不同的subsystem关联，一个进程可以属于多颗树，即**一个进程可以属于多个进程组，只是这些进程组和不同的subsystem关联**。

目前Linux支持12种subsystem，如果不考虑不与任何subsystem关联的情况 (systemd就属于这种情况)，Linux里面最多可以建12颗cgroup树，每棵树关联一个subsystem，当然也可以只建一棵树，然后让这棵树关联所有的subsystem。
当一颗cgroup树不和任何subsystem关联的时候，意味着这棵树只是将进程进行分组，至于要在分组的基础上做些什么，将由应用程序自己决定，systemd就是一个这样的例子。


# `epoll`

在这里，`"事件"`指的是：**发生在文件描述符上的某种状态变化或操作**，例如数据可读、数据可写、连接建立等。
在使用 I/O 复用模型（如 epoll）时，程序会监听多个文件描述符，当某个文件描述符上发生了特定的事件时，操作系统会通知程序，程序可以通过处理这些事件来进行相应的操作。

事件和文件描述符的关系可以通过一个简单的例子来说明：
假设有一个服务器程序，使用 epoll 来同时监听多个客户端的连接。
每个客户端连接都会有一个对应的文件描述符，服务器程序会将这些文件描述符添加到 epoll 实例中进行监听。
当客户端发送数据到服务器时，服务器程序会收到数据可读的事件，然后通过相应的文件描述符来处理这些数据。
具体来说，假设有两个客户端连接，分别对应文件描述符 fd1 和 fd2。服务器程序使用 epoll 监听这两个文件描述符，当 fd1 上有数据可读时，操作系统会将数据可读的事件通知给服务器程序，服务器程序通过 fd1 来接收和处理客户端1发送的数据。同理，当 fd2 上有数据可读时，操作系统会将数据可读的事件通知给服务器程序，服务器程序通过 fd2 来接收和处理客户端2发送的数据。
因此，事件和文件描述符之间的关系是：事件是发生在特定文件描述符上的状态变化或操作，程序通过监听文件描述符上的事件来实现对不同连接或操作的处理。

注意：只要有`任何一个事件`发生，`epoll_wait`就会立即返回。
```cpp
#include <iostream>
#include <sys/socket.h>
#include <sys/epoll.h>
#include <unistd.h>
#include <cstring>

# ifdef ABC    // epoll.h中的结构体定义
typedef union epoll_data
{
  void *ptr;   // 指向用户自定义的、任意类型的数据
  int fd;   // 表示文件描述符
  uint32_t u32;
  uint64_t u64;
} epoll_data_t;

struct epoll_event
{
  uint32_t events;	/* Epoll events */   // 定义事件类型
  epoll_data_t data;	/* User data variable */
} __EPOLL_PACKED;

# endif

struct Data {
    int id;
    char message[256];
};

void* threadB(void* arg) {
    int* sockfd = (int*)arg;
    struct epoll_event event;   // epoll_event类型用于描述事件：事件类型 / 事件数据
    struct epoll_event events[1];
    int epollfd = epoll_create(1);  // 创建epoll实例，监听的文件描述符数量设置为1
    event.events = EPOLLIN; // EPOLLIN表示关注可读事件,文件描述符上有数据可读
    event.data.fd = *sockfd;    // 设置了event结构体的data字段，存储了要监听的文件描述符，即socket
    epoll_ctl(epollfd, EPOLL_CTL_ADD, *sockfd, &event);     // 这行代码将线程B的socket文件描述符添加到epoll实例中进行监听
    // epoll_ctl函数用于控制epoll实例，EPOLL_CTL_ADD表示添加一个新的文件描述符到epoll实例中，*sockfd是要添加的文件描述符，&event是描述事件的结构体。

    while (true) {
        int nfds = epoll_wait(epollfd, events, 1, -1);  // -1 表示不设置超时时间，nfds存储了发生事件的文件描述符的数量（既然在epoll_ctl里设置了要监听的文件描述符，为什么还要判断：因为实际情况中，会同时监听多个文件描述符）
        for (int i = 0; i < nfds; ++i) {
            if (events[i].data.fd == *sockfd) {     // 遍历发生事件的文件描述符，判断是否是线程B的socket文件描述符
                Data data;
                recv(*sockfd, &data, sizeof(data), 0);
                std::cout << "Received data from A - ID: " << data.id << ", Message: " << data.message << std::endl;
            }
        }
    }
}

int main() {
    int sv[2];
    socketpair(AF_UNIX, SOCK_STREAM, 0, sv);    // 使用socketpair函数创建了一个全双工的通信管道

    pthread_t tid;
    pthread_create(&tid, NULL, threadB, &sv[1]);

    Data data = {123, "Hello from A"};
    send(sv[0], &data, sizeof(data), 0);

    pthread_join(tid, NULL);
    close(sv[0]);
    close(sv[1]);

    return 0;
}

```

